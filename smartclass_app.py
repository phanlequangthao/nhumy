import streamlit as st
import av
import cv2
import numpy as np
import pandas as pd
import os
import pickle
import face_recognition
from datetime import datetime
import time
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode
from pathlib import Path
import matplotlib.pyplot as plt
from typing import Dict, List, Union

# C·∫•u h√¨nh trang
st.set_page_config(page_title="SmartClass AI", layout="wide")
st.title("üéì SmartClass AI ‚Äì Tr·ª£ L√Ω L·ªõp H·ªçc Th√¥ng Minh v2.0")

# C√°c h·∫±ng s·ªë v√† thi·∫øt l·∫≠p
FACE_DISTANCE_THRESHOLD = 0.55  # Ng∆∞·ª°ng nh·∫≠n di·ªán khu√¥n m·∫∑t (gi√° tr·ªã c√†ng th·∫•p c√†ng ch·∫∑t ch·∫Ω)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# Kh·ªüi t·∫°o session state
if 'running' not in st.session_state:
    st.session_state.running = False
if 'detection_threshold' not in st.session_state:
    st.session_state.detection_threshold = FACE_DISTANCE_THRESHOLD
if 'analysis_started' not in st.session_state:
    st.session_state.analysis_started = datetime.now()
if 'statistics' not in st.session_state:
    st.session_state.statistics = {}

# Load danh s√°ch h·ªçc sinh ƒë√£ hu·∫•n luy·ªán
def load_students():
    if os.path.exists("students.pkl"):
        with open("students.pkl", "rb") as f:
            try:
                return pickle.load(f)
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu h·ªçc sinh: {e}")
                return {}
    return {}

# L∆∞u th·ªëng k√™ ƒëi k√®m v√†o file CSV
def mark_attendance(name):
    df = pd.read_csv("diemdanh.csv") if os.path.exists("diemdanh.csv") else pd.DataFrame(columns=["Name", "Time"])
    if name not in df["Name"].values:
        now = datetime.now()
        time_str = now.strftime('%H:%M:%S')
        df.loc[len(df)] = [name, time_str]
        df.to_csv("diemdanh.csv", index=False)

# ƒêi·ªÅu ch·ªânh ƒë·ªô s√°ng c·ªßa h√¨nh ·∫£nh
def adjust_brightness(image, factor=1.0):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hsv = np.array(hsv, dtype=np.float64)
    hsv[:, :, 2] = np.clip(hsv[:, :, 2] * factor, 0, 255)
    hsv = np.array(hsv, dtype=np.uint8)
    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

# H√†m so s√°nh khu√¥n m·∫∑t v·ªõi danh s√°ch h·ªçc sinh
def compare_face_with_students(face_encoding, students, threshold=FACE_DISTANCE_THRESHOLD):
    if not face_encoding or not students:
        return None, float('inf')
    
    best_match = None
    best_distance = threshold
    
    for student in students.values():
        # Ch·ªâ ki·ªÉm tra t·ªëi ƒëa 3 encoding ƒë·∫ßu ti√™n c·ªßa h·ªçc sinh ƒë·ªÉ tƒÉng t·ªëc
        for encoding in student.face_encodings[:3]:
            if encoding is not None:
                # T√≠nh kho·∫£ng c√°ch Euclide cho nhanh
                face_distance = np.linalg.norm(np.array(face_encoding) - np.array(encoding))
                face_distance = face_distance / 128.0  # Chu·∫©n h√≥a
                if face_distance < best_distance:
                    best_distance = face_distance
                    best_match = student
    
    return best_match, best_distance

# L·ªõp x·ª≠ l√Ω WebRTC cho video
class VideoProcessor(VideoProcessorBase):
    def __init__(self, students: Dict, threshold: float = FACE_DISTANCE_THRESHOLD):
        self.students = students
        self.threshold = threshold
        self.status_dict = {}
        self.last_processed_time = time.time()
        self.process_every_n_frames = 5  # TƒÉng t·ª´ 2 l√™n 5 ƒë·ªÉ tƒÉng t·ªëc
        self.frame_counter = 0
        # Pre-allocate face locations ƒë·ªÉ t√°i s·ª≠ d·ª•ng
        self.face_locations = []
        
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        self.frame_counter += 1
        
        # Ch·ªâ x·ª≠ l√Ω m·ªói n frame
        if self.frame_counter % self.process_every_n_frames != 0:
            # Tr·∫£ v·ªÅ frame tr·ª±c ti·∫øp kh√¥ng x·ª≠ l√Ω ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian
            return frame
            
        # Chuy·ªÉn ƒë·ªïi frame sang ƒë·ªãnh d·∫°ng h√¨nh ·∫£nh OpenCV
        img = frame.to_ndarray(format="bgr24")
        
        # Th·ªùi gian hi·ªán t·∫°i
        current_time = time.time()
        
        # Resize ·∫£nh nh·ªè h∆°n ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n
        h, w = img.shape[:2]
        # Gi·∫£m k√≠ch th∆∞·ªõc nhi·ªÅu h∆°n, ch·ªâ c√≤n 320 pixel theo chi·ªÅu r·ªông
        scale = 320 / w
        img_small = cv2.resize(img, (0, 0), fx=scale, fy=scale)
            
        # Ph√°t hi·ªán khu√¥n m·∫∑t v√† t√≠nh encoding
        rgb_small = cv2.cvtColor(img_small, cv2.COLOR_BGR2RGB)
        # S·ª≠ d·ª•ng HOG model nhanh h∆°n CNN (m·∫∑c ƒë·ªãnh)
        face_locations = face_recognition.face_locations(rgb_small, model="hog")
        
        if face_locations:
            face_encodings = face_recognition.face_encodings(rgb_small, face_locations, num_jitters=1)
            
            # ƒê·∫£m b·∫£o ch·ªâ x·ª≠ l√Ω khi c√≥ face_encodings
            if len(face_encodings) > 0:
                small_to_orig_ratio = img.shape[0] / img_small.shape[0]
                
                # X·ª≠ l√Ω t·ª´ng khu√¥n m·∫∑t
                for i, (top, right, bottom, left) in enumerate(face_locations):
                    if i < len(face_encodings):
                        # Chuy·ªÉn t·ªça ƒë·ªô v·ªÅ khung h√¨nh g·ªëc
                        top = int(top * small_to_orig_ratio)
                        right = int(right * small_to_orig_ratio)
                        bottom = int(bottom * small_to_orig_ratio)
                        left = int(left * small_to_orig_ratio)
                        
                        # T√¨m h·ªçc sinh ph√π h·ª£p nh·∫•t
                        best_match, distance = compare_face_with_students(
                            face_encodings[i], self.students, self.threshold
                        )
                        
                        # V·∫Ω bounding box
                        if best_match:
                            # C·∫≠p nh·∫≠t tr·∫°ng th√°i h·ªçc sinh
                            status = best_match.update_status("Tap trung")  # Default l√† t·∫≠p trung
                            
                            # M√†u s·∫Øc d·ª±a tr√™n tr·∫°ng th√°i
                            color = (0, 255, 0) if status == "Tap trung" else (0, 0, 255)
                            
                            # V·∫Ω h·ªôp v√† t√™n (ƒë∆°n gi·∫£n h√≥a)
                            cv2.rectangle(img, (left, top), (right, bottom), color, 2)
                            cv2.putText(img, best_match.name, 
                                      (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 
                                      0.7, color, 2)
                            
                            # Ghi nh·∫≠n ƒëi·ªÉm danh
                            mark_attendance(best_match.name)
                            
                            # L∆∞u th√¥ng tin tr·∫°ng th√°i
                            self.status_dict[best_match.name] = status
                        else:
                            # V·∫Ω h·ªôp ƒë·ªè cho ng∆∞·ªùi l·∫°
                            cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)
        
        # C·∫≠p nh·∫≠t tr·∫°ng th√°i to√†n c·ª•c ƒë·ªÉ hi·ªÉn th·ªã
        st.session_state.statistics = self.status_dict
        
        # Th√™m FPS hi·ªÉn th·ªã
        fps = 1.0 / (time.time() - self.last_processed_time) * self.process_every_n_frames
        cv2.putText(img, f"FPS: {fps:.1f}", (10, 30), 
                  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        self.last_processed_time = time.time()
        
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# Sidebar cho c√†i ƒë·∫∑t v√† th√¥ng tin h·ªá th·ªëng
st.sidebar.title("‚öôÔ∏è C√†i ƒë·∫∑t")
detection_threshold = st.sidebar.slider(
    "Ng∆∞·ª°ng nh·∫≠n di·ªán khu√¥n m·∫∑t", 
    min_value=0.3, 
    max_value=0.9, 
    value=st.session_state.detection_threshold,
    step=0.05,
    help="Gi√° tr·ªã th·∫•p h∆°n cho ph√©p nh·∫≠n di·ªán d·ªÖ d√†ng h∆°n nh∆∞ng c√≥ th·ªÉ g√¢y nh·∫ßm l·∫´n. Gi√° tr·ªã cao h∆°n y√™u c·∫ßu ƒë·ªô ch√≠nh x√°c cao h∆°n."
)

# C·∫≠p nh·∫≠t ng∆∞·ª°ng ph√°t hi·ªán
if detection_threshold != st.session_state.detection_threshold:
    st.session_state.detection_threshold = detection_threshold
    FACE_DISTANCE_THRESHOLD = detection_threshold

# Tab cho c√°c t√≠nh nƒÉng kh√°c nhau
tab1, tab2, tab3 = st.tabs(["üìπ Gi√°m s√°t l·ªõp h·ªçc", "üìä Th·ªëng k√™", "‚ö° Hu·∫•n luy·ªán"])

# Tab gi√°m s√°t l·ªõp h·ªçc
with tab1:
    st.subheader("üìπ Gi√°m s√°t l·ªõp h·ªçc - WebRTC")
    students = load_students()
    
    # Hi·ªÉn th·ªã danh s√°ch h·ªçc sinh ƒë√£ hu·∫•n luy·ªán
    st.sidebar.subheader("üë®‚Äçüéì Danh s√°ch h·ªçc sinh ƒë√£ hu·∫•n luy·ªán:")
    if students:
        for student_name, student in students.items():
            num_encodings = len(student.face_encodings)
            st.sidebar.write(f"- **{student_name}**: {num_encodings} m·∫´u khu√¥n m·∫∑t")
    else:
        st.sidebar.warning("‚ö†Ô∏è Ch∆∞a c√≥ h·ªçc sinh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán!")
    
    # Chia giao di·ªán th√†nh 2 c·ªôt
    col_video, col_status = st.columns([3, 1])  # Thay ƒë·ªïi t·ªâ l·ªá ƒë·ªÉ video l·ªõn h∆°n
    
    # V√πng hi·ªÉn th·ªã WebRTC video
    with col_video:
        ctx = webrtc_streamer(
            key="smartclass",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=RTC_CONFIGURATION,
            video_processor_factory=lambda: VideoProcessor(students, FACE_DISTANCE_THRESHOLD),
            media_stream_constraints={
                "video": {"width": {"ideal": 640}, "height": {"ideal": 480}},  # Gi·∫£m k√≠ch th∆∞·ªõc video
                "audio": False
            },
            async_processing=True,
        )
    
    # V√πng hi·ªÉn th·ªã tr·∫°ng th√°i h·ªçc sinh
    with col_status:
        status_placeholder = st.empty()
        attention_placeholder = st.empty()
        
        # Hi·ªÉn th·ªã b·∫£ng tr·∫°ng th√°i
        if st.session_state.statistics:
            df = pd.DataFrame(list(st.session_state.statistics.items()), columns=["T√™n", "Tr·∫°ng th√°i"])
            status_placeholder.dataframe(df, hide_index=True)
            
            # Hi·ªÉn th·ªã ƒë√°nh gi√° m·ª©c ƒë·ªô t·∫≠p trung
            attention_stats = {}
            for name, student_obj in students.items():
                if name in st.session_state.statistics:
                    attention_stats[name] = {
                        "ƒêi·ªÉm t·∫≠p trung": int(student_obj.attention_score),
                        "% Th·ªùi gian t·∫≠p trung": f"{student_obj.get_attention_percentage():.1f}%"
                    }
            
            if attention_stats:
                attention_df = pd.DataFrame.from_dict(attention_stats, orient='index')
                attention_placeholder.dataframe(attention_df, hide_index=False)
        else:
            status_placeholder.info("üëÜ B·∫•m 'START' ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√°t hi·ªán khu√¥n m·∫∑t")

# Tab th·ªëng k√™
with tab2:
    st.subheader("üìä Th·ªëng k√™ ƒë∆°n gi·∫£n")
    
    # Hi·ªÉn th·ªã th·ªùi gian ph√¢n t√≠ch
    analysis_duration = datetime.now() - st.session_state.analysis_started
    st.write(f"‚è±Ô∏è Th·ªùi gian ph√¢n t√≠ch: {analysis_duration.total_seconds():.0f} gi√¢y")
    
    if students:
        # Hi·ªÉn th·ªã b·∫£ng th·ªëng k√™ ƒë∆°n gi·∫£n
        st.write("### B·∫£ng th·ªëng k√™ t·∫≠p trung")
        
        # T·∫°o b·∫£ng th·ªëng k√™ ƒë∆°n gi·∫£n
        stats_data = []
        for name, student in students.items():
            if not hasattr(student, 'attention_score'):
                student.attention_score = 100
            
            stats_data.append({
                "H·ªçc sinh": name, 
                "Tr·∫°ng th√°i": student.last_status,
                "ƒêi·ªÉm t·∫≠p trung": f"{student.attention_score:.0f}/100"
            })
        
        if stats_data:
            stats_df = pd.DataFrame(stats_data)
            st.dataframe(stats_df, hide_index=True)
        else:
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu th·ªëng k√™")
        
        # N√∫t reset th·ªëng k√™
        if st.button("üîÑ ƒê·∫∑t l·∫°i th·ªëng k√™"):
            for student in students.values():
                student.reset_statistics()
            st.success("‚úÖ ƒê√£ ƒë·∫∑t l·∫°i th·ªëng k√™ cho t·∫•t c·∫£ h·ªçc sinh!")
            st.session_state.analysis_started = datetime.now()
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ h·ªçc sinh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ hi·ªÉn th·ªã th·ªëng k√™")

# Tab hu·∫•n luy·ªán
with tab3:
    st.subheader("‚ö° Hu·∫•n luy·ªán m√¥ h√¨nh")
    st.write("""
    #### H∆∞·ªõng d·∫´n hu·∫•n luy·ªán:
    1. ƒê·∫∑t video c·ªßa h·ªçc sinh v√†o th∆∞ m·ª•c `dataset/[t√™n h·ªçc sinh]/`
    2. Ch·∫°y l·ªánh `python train_model.py` ƒë·ªÉ hu·∫•n luy·ªán
    3. Ho·∫∑c s·ª≠ d·ª•ng form d∆∞·ªõi ƒë√¢y ƒë·ªÉ hu·∫•n luy·ªán nhanh
    """)
    
    # Form hu·∫•n luy·ªán t·ª´ file c√≥ s·∫µn
    with st.form("train_form"):
        student_name = st.text_input("T√™n h·ªçc sinh:")
        video_path = st.text_input("ƒê∆∞·ªùng d·∫´n ƒë·∫øn video/·∫£nh:")
        
        submitted = st.form_submit_button("üöÄ Hu·∫•n luy·ªán")
        if submitted:
            if student_name and video_path:
                if os.path.exists(video_path):
                    st.info(f"‚è≥ ƒêang hu·∫•n luy·ªán cho h·ªçc sinh {student_name}...")
                    
                    # Import h√†m train_student t·ª´ train_model.py
                    from train_model import train_student
                    try:
                        student = train_student(student_name, [video_path])
                        st.success(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng cho {student_name} v·ªõi {len(student.face_encodings)} m·∫´u khu√¥n m·∫∑t")
                        st.experimental_rerun()
                    except Exception as e:
                        st.error(f"‚ùå L·ªói khi hu·∫•n luy·ªán: {str(e)}")
                else:
                    st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file video/·∫£nh: {video_path}")
            else:
                st.error("‚ùå Vui l√≤ng ƒëi·ªÅn ƒë·∫ßy ƒë·ªß th√¥ng tin")
    
    # Hi·ªÉn th·ªã m·∫´u khu√¥n m·∫∑t ƒë√£ hu·∫•n luy·ªán
    st.subheader("üëÅÔ∏è M·∫´u khu√¥n m·∫∑t ƒë√£ hu·∫•n luy·ªán")
    
    for name in students.keys():
        sample_dir = Path(f"dataset/{name}")
        if sample_dir.exists():
            st.write(f"#### üßë‚Äçüéì {name}")
            
            # L·∫•y t·ªëi ƒëa 5 m·∫´u khu√¥n m·∫∑t
            samples = list(sample_dir.glob("*.jpg"))[:5]
            if not samples:
                samples = list(sample_dir.glob("*.jpeg"))[:5]
                
            if samples:
                cols = st.columns(min(5, len(samples)))
                for i, sample_path in enumerate(samples):
                    with cols[i]:
                        sample_img = cv2.imread(str(sample_path))
                        if sample_img is not None:
                            st.image(cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB), 
                                    caption=f"M·∫´u {i+1}",
                                    width=100)
            else:
                st.info(f"Kh√¥ng t√¨m th·∫•y m·∫´u khu√¥n m·∫∑t cho {name}")

st.markdown("---")
st.caption("¬© 2023 SmartClass AI - Tr·ª£ l√Ω l·ªõp h·ªçc th√¥ng minh | Phi√™n b·∫£n 2.0 (WebRTC)")

# Th√™m th√¥ng tin FPS
st.sidebar.write("#### ‚ö° Th√¥ng tin hi·ªáu su·∫•t")
st.sidebar.info("""
WebRTC cho ph√©p x·ª≠ l√Ω video tr·ª±c ti·∫øp trong tr√¨nh duy·ªát, gi√∫p tƒÉng FPS ƒë√°ng k·ªÉ so v·ªõi ph∆∞∆°ng ph√°p x·ª≠ l√Ω tr√™n server.
- X·ª≠ l√Ω m·ªói frame th·ª© 2
- Gi·∫£m k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ph√¢n t√≠ch ƒë·ªÉ tƒÉng t·ªëc
- Gi·∫£m thi·ªÉu vi·ªác truy·ªÅn d·ªØ li·ªáu l√™n server
""")